<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Slms on AI Notes</title>
    <link>https://notes.muthu.co/tags/slms/</link>
    <description>Recent content in Slms on AI Notes</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 08 Jul 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://notes.muthu.co/tags/slms/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Small Language Models are the Future of Agentic AI</title>
      <link>https://notes.muthu.co/2025/07/small-language-models-are-the-future-of-agentic-ai/</link>
      <pubDate>Tue, 08 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://notes.muthu.co/2025/07/small-language-models-are-the-future-of-agentic-ai/</guid>
      <description>&lt;p&gt;I recently came across a paper titled &lt;em&gt;“Small Language Models are the Future of Agentic AI,”&lt;/em&gt; and it got me thinking. The message is simple but powerful: bigger isn&amp;rsquo;t always better.&lt;/p&gt;&#xA;&lt;p&gt;In the current AI landscape, we often assume that more power equals more performance. But this paper challenges that assumption. Instead, it offers a smarter and more strategic view of how AI can scale without scaling costs.&lt;/p&gt;&#xA;&lt;p&gt;Let’s break it down.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
