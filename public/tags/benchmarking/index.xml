<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Benchmarking on Engineering Notes</title>
    <link>https://notes.muthu.co/tags/benchmarking/</link>
    <description>Recent content in Benchmarking on Engineering Notes</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 19 Feb 2026 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://notes.muthu.co/tags/benchmarking/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Agent Evaluation and Benchmarking for Measuring What Matters</title>
      <link>https://notes.muthu.co/2026/02/agent-evaluation-and-benchmarking-for-measuring-what-matters/</link>
      <pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://notes.muthu.co/2026/02/agent-evaluation-and-benchmarking-for-measuring-what-matters/</guid>
      <description>&lt;p&gt;You built an AI agent. It works on your demos. But is it actually &lt;em&gt;good&lt;/em&gt;? Can it handle real-world complexity? Will it break on edge cases? Agent evaluation is one of the hardest unsolved problems in the field — and one of the most important. Without rigorous evaluation, you&amp;rsquo;re flying blind. This article covers the principles, metrics, benchmarks, and practical frameworks for measuring agent performance systematically.&lt;/p&gt;&#xA;&lt;h2 id=&#34;1-concept-introduction&#34;&gt;1. Concept Introduction&lt;/h2&gt;&#xA;&lt;h3 id=&#34;simple-explanation&#34;&gt;Simple Explanation&lt;/h3&gt;&#xA;&lt;p&gt;Think of agent evaluation like grading a student. A multiple-choice exam (traditional ML benchmarks) tests one narrow skill. But agents are more like interns — they perform multi-step tasks, use tools, make judgment calls, and recover from mistakes. You need a richer evaluation framework: not just &amp;ldquo;did you get the right answer?&amp;rdquo; but &amp;ldquo;did you take reasonable steps, use resources efficiently, and handle surprises gracefully?&amp;rdquo;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Why MMLU Matters and What Massive Multitask Language Understanding Really Tests</title>
      <link>https://notes.muthu.co/2025/07/why-mmlu-matters-and-what-massive-multitask-language-understanding-really-tests/</link>
      <pubDate>Wed, 16 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://notes.muthu.co/2025/07/why-mmlu-matters-and-what-massive-multitask-language-understanding-really-tests/</guid>
      <description>&lt;p&gt;I remember the first time I heard about MMLU. I was reading about GPT-3&amp;rsquo;s capabilities, and there it was - this benchmark that claimed to test AI across 57 different subjects. From elementary math to professional law, from world history to computer science. It sounded almost too ambitious to be real.&lt;/p&gt;&#xA;&lt;p&gt;But here&amp;rsquo;s the thing about MMLU (Measuring Massive Multitask Language Understanding) - it&amp;rsquo;s become one of the most important ways we measure how smart our AI systems really are. And after diving deep into it, I think it&amp;rsquo;s worth understanding what makes it special.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
