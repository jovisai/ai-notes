<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>A Technical Introduction to Ava - Engineering Notes</title><meta name="viewport" content="width=device-width, initial-scale=1">
	
  <meta itemprop="name" content="A Technical Introduction to Ava">
  <meta itemprop="description" content="Ava is an AI voice assistant that runs entirely in the browser. It is an experimental project to understand how to use LLM models completely on the client side and use it with speech. It uses WebAssembly to perform all of its functions locally on the user’s device which means that no data is ever sent to a server, ensuring complete privacy.
GitHub Repository: https://github.com/muthuspark/ava/
Live Demo: https://ava.muthu.co
Ava’s capabilities include:">
  <meta itemprop="datePublished" content="2025-12-22T00:00:00+00:00">
  <meta itemprop="dateModified" content="2025-12-22T00:00:00+00:00">
  <meta itemprop="wordCount" content="1109"><meta property="og:url" content="https://notes.muthu.co/2025/12/a-technical-introduction-to-ava/">
  <meta property="og:site_name" content="Engineering Notes">
  <meta property="og:title" content="A Technical Introduction to Ava">
  <meta property="og:description" content="Ava is an AI voice assistant that runs entirely in the browser. It is an experimental project to understand how to use LLM models completely on the client side and use it with speech. It uses WebAssembly to perform all of its functions locally on the user’s device which means that no data is ever sent to a server, ensuring complete privacy.
GitHub Repository: https://github.com/muthuspark/ava/
Live Demo: https://ava.muthu.co
Ava’s capabilities include:">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-12-22T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-12-22T00:00:00+00:00">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="A Technical Introduction to Ava">
  <meta name="twitter:description" content="Ava is an AI voice assistant that runs entirely in the browser. It is an experimental project to understand how to use LLM models completely on the client side and use it with speech. It uses WebAssembly to perform all of its functions locally on the user’s device which means that no data is ever sent to a server, ensuring complete privacy.
GitHub Repository: https://github.com/muthuspark/ava/
Live Demo: https://ava.muthu.co
Ava’s capabilities include:">
<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link href="https://fonts.googleapis.com/css2?family=Source+Sans+3:ital,wght@0,200..900;1,200..900&display=swap" rel="stylesheet">
	<link rel="stylesheet" type="text/css" media="screen" href="https://notes.muthu.co/css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="https://notes.muthu.co/css/main.css" />

	<link id="dark-scheme" rel="stylesheet" type="text/css" href="https://notes.muthu.co/css/dark.css" /><script src="https://notes.muthu.co/js/feather.min.js"></script>
	
	<script src="https://notes.muthu.co/js/main.js"></script>
</head>


<body>


	
	<div class="container wrapper">
		<div class="header">
	
		<div class="avatar">
			<a href="https://notes.muthu.co/">
				<img src="/avatar.jpeg" alt="Engineering Notes" />
			</a>
		</div>
	
	<h1 class="site-title"><a href="https://notes.muthu.co/">Engineering Notes</a></h1>
	<div class="site-description"><p>Thoughts and Ideas on AI by Muthukrishnan</p><nav class="nav social">
			<ul class="flat"><li><a href="https://github.com/muthuspark/" title="Github"><i data-feather="github"></i></a></li><li><a href="https://linkedin.com/in/krimuthu/" title="LinkedIn"><i data-feather="linkedin"></i></a></li></ul>
		</nav>
	</div>

	<nav class="nav">
		<ul class="flat">
			
			<li>
				<a href="/">Home</a>
			</li>
			
			<li>
				<a href="/posts">All posts</a>
			</li>
			
			<li>
				<a href="/about">About</a>
			</li>
			
			<li>
				<a href="/tags">Tags &amp; Stats</a>
			</li>
			
		</ul>
	</nav>
</div>


		<div class="post">
    <div class="post-header">
    <div class="matter">
        <h1 class="title">A Technical Introduction to Ava</h1>
        
        <div class="date">
            <span class="day">22</span>
            <span class="rest">Dec 2025</span>
        </div>
        
    </div>
</div>


    
    
    <p>
  <img src="https://raw.githubusercontent.com/muthuspark/ava/main/screenshot.png" alt="&lsquo;AVA UI&rsquo;">

</p>
<p>Ava is an AI voice assistant that runs entirely in the browser. It is an experimental project to understand how to use LLM models completely on the client side and use it with speech. It uses WebAssembly to perform all of its functions locally on the user&rsquo;s device which means that no data is ever sent to a server, ensuring complete privacy.</p>
<p><strong>GitHub Repository:</strong> <a href="https://github.com/muthuspark/ava/">https://github.com/muthuspark/ava/</a></p>
<p><strong>Live Demo:</strong> <a href="https://ava.muthu.co">https://ava.muthu.co</a></p>
<p>Ava&rsquo;s capabilities include:</p>
<ul>
<li><strong>Voice Activity Detection (VAD):</strong> Detects when the user is speaking and when they have finished.</li>
<li><strong>Speech-to-Text:</strong> Transcribes the user&rsquo;s speech into text.</li>
<li><strong>Language Model:</strong> Generates a response based on the user&rsquo;s query.</li>
<li><strong>Text-to-Speech:</strong> Converts the generated response into speech.</li>
</ul>
<p>This document provides a technical overview of Ava&rsquo;s architecture, technical stack, and configuration.</p>
<h2 id="architecture">Architecture</h2>
<p>Ava uses a three-stage pipeline architecture, with each stage is powered by WebAssembly-based components.</p>
<pre class="mermaid">graph TB
    %% Input Stage
    MIC[Microphone Input]
    
    %% Stage 1: Speech Recognition
    subgraph SR[&#34;Stage 1: Speech Recognition&#34;]
        direction TB
        VAD[Voice Activity Detection &lt;br&gt; Detects speech segments]
        ASR[Speech-to-Text Transcription &lt;br&gt; Converts audio to text]
        VAD --&gt; ASR
    end
    
    %% Stage 2: Language Model
    subgraph LM[&#34;Stage 2: Language Model&#34;]
        direction TB
        INF[LLM Inference&lt;br&gt;Generates contextual response]
    end
    
    %% Stage 3: Speech Synthesis
    subgraph TTS[&#34;Stage 3: Speech Synthesis&#34;]
        direction TB
        SYNTH[Text-to-Speech&lt;br&gt;Low-latency audio output]
    end
    
    %% Output Stage
    SPEAKER[Speaker Output]
    
    %% Flow connections
    MIC --&gt; SR
    SR --&gt;|Transcribed Text| LM
    LM --&gt;|Generated Response| TTS
    TTS --&gt; SPEAKER
    
    %% Styling
    classDef inputOutput fill:#e1f5ff,stroke:#0288d1,stroke-width:2px
    classDef stage fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef component fill:#fff3e0,stroke:#f57c00,stroke-width:1px
    
    class MIC,SPEAKER inputOutput
    class SR,LM,TTS stage
    class VAD,ASR,INF,SYNTH component
  </pre>

<ol>
<li>
<p><strong>Speech Recognition:</strong> This initial stage captures and processes audio from the user&rsquo;s microphone.</p>
<ul>
<li><strong>Voice Activity Detection (VAD):</strong> Using the Silero VAD model, Ava listens for speech activity. It determines when the user starts and stops speaking, which is a more natural approach than fixed-interval processing.</li>
<li><strong>Transcription:</strong> Once the VAD detects the end of speech, the captured audio segment is transcribed into text using the Whisper (tiny-en) model, which is run via <code>Transformers.js</code>.</li>
</ul>
</li>
<li>
<p><strong>Language Model:</strong> The transcribed text from the previous stage is then fed into the language model.</p>
<ul>
<li><strong>Inference:</strong> Ava uses the Gemma 3 270M model, running on <code>Wllama</code> (a llama.cpp WASM port), to generate a response. The <code>useConversation.ts</code> composable manages this process, triggering inference and streaming the generated tokens.</li>
</ul>
</li>
<li>
<p><strong>Speech Synthesis:</strong> The final stage converts the generated text response back into speech.</p>
<ul>
<li><strong>Low-Latency Output:</strong> The response text is split into sentences at punctuation boundaries (<code>.</code>, <code>!</code>, <code>?</code>, <code>,</code>). Each sentence is then queued for synthesis using the browser&rsquo;s native <code>SpeechSynthesis</code> API. This allows Ava to start speaking before the entire response has been generated, providing a more interactive experience.</li>
</ul>
</li>
</ol>
<h2 id="technical-stack">Technical Stack</h2>
<p>The following table details the technologies used in Ava:</p>
<table>
  <thead>
      <tr>
          <th>Component</th>
          <th>Technology</th>
          <th>Size</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Voice Activity Detection</td>
          <td>Silero VAD v5 (@ricky0123/vad-web)</td>
          <td>~2MB</td>
      </tr>
      <tr>
          <td>Speech-to-Text</td>
          <td>Whisper tiny-en (@huggingface/transformers)</td>
          <td>~40MB</td>
      </tr>
      <tr>
          <td>LLM</td>
          <td>Gemma 3 270M Instruct (Wllama)</td>
          <td>~180MB</td>
      </tr>
      <tr>
          <td>Text-to-Speech</td>
          <td>Web Speech Synthesis API</td>
          <td>Native</td>
      </tr>
      <tr>
          <td>Audio Visualization</td>
          <td>Web Audio API</td>
          <td>Native</td>
      </tr>
      <tr>
          <td>Frontend</td>
          <td>Vue 3 + TypeScript</td>
          <td>—</td>
      </tr>
      <tr>
          <td>Build</td>
          <td>Vite</td>
          <td>—</td>
      </tr>
  </tbody>
</table>
<h2 id="project-structure">Project Structure</h2>
<p>The project is structured as follows:</p>
<pre tabindex="0"><code>src/
├── App.vue                      # Main application shell
├── components/
│   ├── AboutPopup.vue           # Info modal
│   └── WaveformVisualizer.vue   # Real-time audio visualization
├── composables/
│   ├── useConversation.ts       # Orchestrates conversation flow
│   ├── useWhisper.ts            # VAD + Whisper speech recognition
│   ├── useWllama.ts             # Gemma LLM inference
│   ├── useSpeechSynthesis.ts    # Browser TTS wrapper
│   └── useAudioVisualizer.ts    # Web Audio frequency analysis
├── styles/
│   └── main.css                 # Global styles
└── types/
    └── index.ts                 # TypeScript definitions
</code></pre><h2 id="workflow">Workflow</h2>
<p>The following diagram illustrates the event-driven data flow between the core composables:</p>
<pre class="mermaid">graph TD;
    subgraph &#34;User Interaction&#34;
        A[Microphone Audio]
    end

    subgraph &#34;Composables&#34;
        B(useAudioVisualizer)
        C(useWhisper)
        D(useConversation)
        E(useWllama)
        F(useSpeechSynthesis)
    end

    subgraph &#34;Browser APIs&#34;
        G[Web Audio API]
        H[SpeechSynthesis API]
    end

    A -- Raw Audio Stream --&gt; G;
    G -- Analyzed Frequency Data --&gt; B;
    A -- Raw Audio Stream --&gt; C;
    C -- Transcribed Text --&gt; D;
    D -- Prompt --&gt; E;
    E -- Generated Tokens --&gt; D;
    D -- Full Sentence --&gt; F;
    F -- Synthesized Speech --&gt; H;

    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#f9f,stroke:#333,stroke-width:2px
    style C fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
    style E fill:#ccf,stroke:#333,stroke-width:2px
    style F fill:#cfc,stroke:#333,stroke-width:2px
    style G fill:#fcf,stroke:#333,stroke-width:2px
    style H fill:#fcf,stroke:#333,stroke-width:2px
  </pre>

<h3 id="explanation-of-the-workflow">Explanation of the Workflow</h3>
<ol>
<li><strong>Audio Input:</strong> The <code>useAudioVisualizer</code> and <code>useWhisper</code> composables both receive the raw audio stream from the microphone via the Web Audio API.</li>
<li><strong>Visualization:</strong> <code>useAudioVisualizer</code> analyzes the frequency data of the audio stream to create the waveform visualization on the UI.</li>
<li><strong>Speech Recognition:</strong> <code>useWhisper</code> processes the audio stream. It uses the Silero VAD model to detect speech and, once the user stops talking, sends the audio to the Whisper model for transcription.</li>
<li><strong>Conversation Orchestration:</strong> The transcribed text is passed to the <code>useConversation</code> composable, which manages the overall conversation flow.</li>
<li><strong>LLM Inference:</strong> <code>useConversation</code> sends the transcribed text as a prompt to the <code>useWllama</code> composable. <code>useWllama</code> then uses the Gemma 3 LLM to generate a response, streaming the tokens back to <code>useConversation</code>.</li>
<li><strong>Speech Synthesis:</strong> As <code>useConversation</code> receives tokens from <code>useWllama</code>, it assembles them into sentences. Once a complete sentence is formed (determined by the <code>SENTENCE_BOUNDARY</code> regex), it&rsquo;s passed to the <code>useSpeechSynthesis</code> composable. <code>useSpeechSynthesis</code> then uses the browser&rsquo;s SpeechSynthesis API to speak the sentence aloud. This process repeats until the entire response has been synthesized.</li>
</ol>
<h2 id="configuration">Configuration</h2>
<p>Ava&rsquo;s behavior can be customized by adjusting the following parameters:</p>
<h3 id="llm-settings-usewllamats">LLM Settings (<code>useWllama.ts</code>)</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-typescript" data-lang="typescript"><span style="display:flex;"><span><span style="color:#1f2328">nPredict</span>: <span style="color:#cf222e">64</span><span style="color:#1f2328">,</span>              <span style="color:#57606a">// Max tokens (lower = faster response)
</span></span></span><span style="display:flex;"><span><span style="color:#57606a"></span><span style="color:#1f2328">temp</span>: <span style="color:#cf222e">0.7</span><span style="color:#1f2328">,</span>                 <span style="color:#57606a">// Sampling temperature
</span></span></span><span style="display:flex;"><span><span style="color:#57606a"></span><span style="color:#1f2328">top_k</span>: <span style="color:#cf222e">40</span><span style="color:#1f2328">,</span>                 <span style="color:#57606a">// Top-k sampling
</span></span></span><span style="display:flex;"><span><span style="color:#57606a"></span><span style="color:#1f2328">top_p</span>: <span style="color:#cf222e">0.9</span><span style="color:#1f2328">,</span>                <span style="color:#57606a">// Nucleus sampling
</span></span></span></code></pre></div><h3 id="vad-settings-usewhisperts">VAD Settings (<code>useWhisper.ts</code>)</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-typescript" data-lang="typescript"><span style="display:flex;"><span><span style="color:#1f2328">positiveSpeechThreshold</span>: <span style="color:#cf222e">0.5</span><span style="color:#1f2328">,</span>  <span style="color:#57606a">// Confidence threshold for speech detection
</span></span></span><span style="display:flex;"><span><span style="color:#57606a"></span><span style="color:#1f2328">negativeSpeechThreshold</span>: <span style="color:#cf222e">0.35</span><span style="color:#1f2328">,</span> <span style="color:#57606a">// Threshold for non-speech
</span></span></span><span style="display:flex;"><span><span style="color:#57606a"></span><span style="color:#1f2328">redemptionMs</span>: <span style="color:#cf222e">800</span><span style="color:#1f2328">,</span>             <span style="color:#57606a">// Wait time after speech ends before triggering
</span></span></span><span style="display:flex;"><span><span style="color:#57606a"></span><span style="color:#1f2328">minSpeechMs</span>: <span style="color:#cf222e">200</span><span style="color:#1f2328">,</span>              <span style="color:#57606a">// Minimum speech duration to consider
</span></span></span><span style="display:flex;"><span><span style="color:#57606a"></span><span style="color:#1f2328">preSpeechPadMs</span>: <span style="color:#cf222e">300</span><span style="color:#1f2328">,</span>           <span style="color:#57606a">// Audio to include before speech detected
</span></span></span></code></pre></div><h3 id="sentence-boundary-usewllamats">Sentence Boundary (<code>useWllama.ts</code>)</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-typescript" data-lang="typescript"><span style="display:flex;"><span><span style="color:#cf222e">const</span> <span style="color:#1f2328">SENTENCE_BOUNDARY</span> <span style="color:#0550ae">=</span> <span style="color:#0a3069">/[.!?,](?:\s|$)/</span>  <span style="color:#57606a">// TTS triggers on punctuation
</span></span></span></code></pre></div><h2 id="requirements">Requirements</h2>
<ul>
<li><strong>Browser</strong>: Chrome 90+ or Edge 90+ (requires <code>SharedArrayBuffer</code>)</li>
<li><strong>Headers</strong>: Cross-Origin Isolation must be enabled on the hosting server:
<pre tabindex="0"><code>Cross-Origin-Opener-Policy: same-origin
Cross-Origin-Embedder-Policy: require-corp
</code></pre></li>
</ul>
<h2 id="permissions">Permissions</h2>
<p>Ava requires the following browser permissions:</p>
<table>
  <thead>
      <tr>
          <th>Permission</th>
          <th>Purpose</th>
          <th>Required</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Microphone</td>
          <td>Voice input for Whisper</td>
          <td>Yes</td>
      </tr>
      <tr>
          <td>Audio Playback</td>
          <td>Text-to-speech output</td>
          <td>Yes</td>
      </tr>
  </tbody>
</table>
<h2 id="performance">Performance</h2>
<ul>
<li><strong>First load</strong>: Downloads ~220MB of models, which are then cached by the browser.</li>
<li><strong>Inference</strong>:
<ul>
<li>VAD runs in real-time.</li>
<li>Whisper transcription takes approximately 0.3-0.5 seconds.</li>
<li>The LLM takes about 1-2 seconds to generate a response.</li>
</ul>
</li>
<li><strong>Memory</strong>: Consumes between 500MB and 1GB of RAM during operation.</li>
<li><strong>WebGPU</strong>: Not yet supported; all processing runs on the CPU via WASM SIMD.</li>
</ul>
<h2 id="future-work">Future Work</h2>
<ul>
<li><strong>WebGPU Support:</strong> Integrating WebGPU would offload processing to the GPU, significantly speeding up inference times.</li>
<li><strong>Improved Speech Synthesis:</strong> While the native browser API is effective, exploring more advanced, natural-sounding TTS options could enhance the user experience.</li>
<li><strong>Conversation Context:</strong> Implementing a mechanism to carry context over multiple turns would allow for more coherent and engaging conversations.</li>
</ul>

    <hr class="footer-separator" />
<div class="tags">
    
    
    
</div>



<div class="back">
    <a href="https://notes.muthu.co/"><span aria-hidden="true">← Back</span></a>
</div>


<div class="back">
    
</div>

</div>

	</div>
	

	<div class="footer wrapper">
	<nav class="nav">
		<div>&copy 2025   Muthukrishnan</div>
		<div>Contact: muthukrishnan.t [at] hotmail [dot] com</div>
	</nav>
</div><script>feather.replace()</script>

<script type="module">
	import mermaid from "https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.esm.min.mjs";

	const getMermaidTheme = () => {
		const savedScheme = localStorage.getItem('scheme');
		if (savedScheme) {
			return savedScheme === 'dark' ? 'dark' : 'default';
		}
		return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
	};

	mermaid.initialize({
		startOnLoad: true,
		theme: getMermaidTheme(),
		handDrawn: true
	});

	const applyTheme = () => {
		const newTheme = getMermaidTheme();
		mermaid.initialize({
			startOnLoad: true,
			theme: newTheme,
			handDrawn: true
		});
	};

	window.addEventListener('storage', (event) => {
		if (event.key === 'scheme') {
			applyTheme();
		}
	});

	window.matchMedia('(prefers-color-scheme: dark)').addEventListener('change', applyTheme);
</script>


	
</body>

</html>
